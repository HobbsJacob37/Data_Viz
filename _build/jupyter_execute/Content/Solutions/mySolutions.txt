from google.colab import auth
auth.authenticate_user()
print('Authenticated')



import pandas as pa
import requests
from bs4 import BeautifulSoup
import re
import matplotlib.pyplot as plt


%%bigquery --project pic-math
WITH t as (
SELECT COUNT(*) as number_trips, start_station_name
FROM `bigquery-public-data.austin_bikeshare.bikeshare_trips`
GROUP BY start_station_name
ORDER BY number_trips DESC
LIMIT 2
)

SELECT *
FROM t
ORDER BY number_trips
LIMIT 1

%%bigquery --project pic-math

SELECT COUNT(*) as round_trips, start_station_name
FROM `bigquery-public-data.austin_bikeshare.bikeshare_trips`
WHERE start_station_name = end_station_name AND duration_minutes >= 60
GROUP BY start_station_name



%%bigquery --project pic-math
WITH stations as (
SELECT name, property_type
FROM `bigquery-public-data.austin_bikeshare.bikeshare_stations`
)

SELECT stations.property_type as starting_station_type, 
       AVG(trips.duration_minutes) as average_ride_minutes, 
       count(*) as number_of_trips, 
       STDDEV_POP(trips.duration_minutes) as std_ride_minutes
FROM `bigquery-public-data.austin_bikeshare.bikeshare_trips` as trips 
    LEFT OUTER JOIN stations 
      ON trips.start_station_name = stations.name
WHERE stations.property_type = 'sidewalk' OR stations.property_type = 'parkland'
GROUP BY stations.property_type
ORDER BY average_ride_minutes DESC



html = requests.get('https://en.wikipedia.org/wiki/List_of_Marvel_Cinematic_Universe_films')
soup = BeautifulSoup(html.text)

df3 = pa.read_html(str(soup.find_all('table', class_='wikitable plainrowheaders')[0]))[0]

df3.columns = df3.columns.droplevel(1)

head = list(df3.columns[:-1])
head.append('Phase')
df3.columns = head

df3.Phase = [1,1,1,1,1,1,0,2,2,2,2,2,2,0,3,3,3,3,3,3,3,3,3,3,3]

df3.drop([6,13])

# RUN THIS CELL WHEN USING THE NOTEBOOK ON COLAB - NO PREVIOUS INSTALLATION OF SELENIUM IS NEEDED
# install chromium, its driver, and selenium
!apt update
!apt install chromium-chromedriver
!pip install selenium
# set options to be headless
from selenium import webdriver
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
# open it, go to a website, and get results
driver = webdriver.Chrome('chromedriver',options=options)
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

url = 'https://google.com'
driver.get(url)


elem = driver.find_element(By.XPATH, '//input')

elem.send_keys("Tottenham Football Club")

#elem = driver.find_element(By.XPATH, '//input[@name = "btnI"]')
elem.send_keys(Keys.ENTER)
driver.current_url

from bs4 import BeautifulSoup
soup = BeautifulSoup(driver.page_source)
soup.title

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support.expected_conditions import staleness_of

old_page = driver.find_element(By.XPATH,'//html')
driver.find_element(By.PARTIAL_LINK_TEXT,'twitter').click()

WebDriverWait(driver, 10).until(staleness_of(old_page))
#driver.implicitly_wait(5)

driver.title



driver.current_url

driver.title

driver.current_url

line = 'happy birthday to you\n'

print(line.capitalize() +  line + line.replace("to you","dear {}".format(input("Enter the birthday celebrant's name: "))) + line)



import requests
import pandas as pa
from bs4 import BeautifulSoup


r = requests.get('https://en.wikipedia.org/wiki/List_of_highest_mountains_on_Earth')
html_contents = r.text
html_soup = BeautifulSoup(html_contents,"lxml")
tables = html_soup.find_all('table',class_="wikitable")

df1 = pa.read_html(str(tables))[0]
df1.columns = df1.columns.droplevel(0).droplevel(0)
df1.head()

cols = df1.columns.map(lambda s: re.sub(r"\[(.+)\]","",s))

cols

re.sub(r"\((.+)\)","",cols[1])

cols = cols.map(lambda s: re.sub(r"\((.+)\)","",s))

cols

re.sub(r" ","_",cols[1])

cols = cols.map(lambda s: s.strip())
cols

cols = cols.map(lambda s: re.sub(r" ","_",s))

cols

cols = cols.map(lambda s : s.lower())

cols

df1.columns = cols

df1.head()

r = requests.get('https://en.wikipedia.org/wiki/List_of_highest_mountains_on_Earth')
html_contents = r.text
html_soup = BeautifulSoup(html_contents,"lxml")
tables = html_soup.find_all('table',class_="wikitable")

df1 = pa.read_html(str(tables))[0]
df1.columns = df1.columns.droplevel(0).droplevel(0)
df1.head()

df1.iloc[:,-1]

newcol = df1.iloc[:,-1]

newcol = newcol.apply(lambda x: re.sub(r"\[(.+?)\]","",x))



newcol

newcol = newcol.apply(lambda x: re.sub(r"[^A-z]","",x))

newcol

newcol = newcol.apply(lambda x: re.findall(r"[A-Z][a-z]*",x))
newcol

max(newcol.apply(lambda x: len(x)))

newcols = pa.DataFrame(newcol.to_list(), columns = ['country1','country2','country3'])

newcols

df1 = pa.concat([df1,newcols], axis = 1)

df1.head()



iot = pa.read_csv('https://raw.githubusercontent.com/nurfnick/Data_Viz/main/IOT-temp.csv')

iot.head()

iot.shape

iot.noted_date = pa.to_datetime(iot.noted_date,format = '%d-%m-%Y %H:%M')

iot.noted_date.max()

iot.noted_date.min()

(iot.noted_date.shift() - iot.noted_date).max()

(iot.noted_date.shift() - iot.noted_date).idxmax()

iot.iloc[14030:14050,:]

iot.noted_date.mean()

iot[iot.noted_date.dt.date == pa.Timestamp('09-11-2018')]

start = pa.Timestamp('09-11-2018')
end = pa.Timestamp('09-12-2018')


iot[iot.noted_date.between(start,end)]

iot[(iot.noted_date.between(start,end))& (iot['out/in'] == 'Out')].temp.mean()

iot[iot.noted_date.between(start,end, inclusive = 'left')]

iot[(iot.noted_date.between(start,end, inclusive = "left"))& (iot['out/in'] == 'Out')].temp.mean()



df = pa.read_csv('https://raw.githubusercontent.com/nurfnick/Data_Viz/main/iris.csv')

df.head()

df.SepalLength.mean()

df.SepalLength.astype('int').mean()

df.groupby('Class').agg('mean')

df2 = df[['PedalLength','PedalWidth','SepalLength','SepalWidth']].astype('int')

df2 = pa.concat([df2,df.Class],axis = 1)

df2.head()

df2.groupby('Class').agg('mean')

df.groupby('Class').agg(['mean','median','count','std'])



import pandas as pa

df3 = pa.read_csv('https://raw.githubusercontent.com/nurfnick/Data_Viz/main/AB_NYC_2019.csv')

!pip install --upgrade matplotlib

ax = df3.groupby('neighbourhood_group').price.agg('max').plot.bar(ylim = [0,11100], 
                                                                  title = 'Max Price by Borough', 
                                                                  ylabel = 'Price in Dollars',
                                                                  rot = 45)

for container in ax.containers:
    ax.bar_label(container)





df3.groupby(['neighbourhood_group','room_type']).price.agg('max').plot.bar(y = 'room_type')



df_pivot = pa.pivot_table(
    df3, 
    values="price",
    index="neighbourhood_group",
    columns="room_type", 
    aggfunc=max
)

df_pivot.plot.bar(title = 'Grouped by Borough')

df_pivot

df_pivot = pa.pivot_table(
    df3, 
    values="price",
    index="room_type",
    columns="neighbourhood_group", 
    aggfunc=max
)

df_pivot.plot.bar(title = 'Grouped by Room Type')

df3.price.plot.hist(title = "Price of Air B&B in NYC with Outliers Removed",bins = 100, xlim = [0,2000]).set_xlabel("Price")


df3.price.plot.hist(title = "Price of Air B&B in NYC with Outliers",bins = 100)

df3.price.plot.hist(title = "Price of Air B&B in NYC with Outliers",bins = 100, logx = True)

df3.groupby('neighbourhood_group').price.plot.hist(alpha = .7, bins = 100, xlim = [0,2000], legend = True, title = "Distribution of Boroughs Pricing with Outliers Removed")

df3.groupby('neighbourhood_group').price.plot.hist(alpha = .5, bins = 100, legend = True, title = "Distribution of Boroughs Pricing loglog Scale", logx = True, logy = True)

plt.show()

import pandas as pa
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

df = pa.read_csv('https://raw.githubusercontent.com/nurfnick/Data_Viz/main/Activity_Dataset_V1.csv')

def func(pct, allvals):
    absolute = int(pct/100.*np.sum(allvals))
    return "{:.0f}%\n{:d}".format(pct, absolute)


plt.pie(x=df.groupby('workout_type').workout_type.agg('count'),
        labels = df.groupby('workout_type').workout_type.agg('count').index, 
        autopct=lambda pct: func(pct,df.groupby('workout_type').workout_type.agg('count')),
        colors = sns.color_palette('bright')[:7])
plt.show()

def func(pct, allvals):
    absolute = int(pct/100.*np.sum(allvals))
    return "{:d}".format(absolute)


plt.pie(x=df.groupby('workout_type').workout_type.agg('count'),
        labels = df.groupby('workout_type').workout_type.agg('count').index, 
        autopct=lambda pct: func(pct,df.groupby('workout_type').workout_type.agg('count')),
        colors = sns.color_palette('colorblind')[:8])
plt.show()


